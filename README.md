## ğŸ§  Human Activity Recognition â€” ML Algorithm Comparison

### ğŸ“˜ Overview

This project compares multiple **machine learning algorithms** on the **HARTH (Human Activity Recognition)** dataset, which contains wearable sensor data from thigh and back IMUs.

The aim is to analyze model performance across **Accuracy, Precision, Recall, F1-Score, and Training Time**, visualize results, and test model predictions interactively using a **Streamlit dashboard**.

---

### ğŸ¯ Objectives

* Compare the performance of classical supervised ML algorithms.
* Visualize results interactively on a Streamlit UI.
* Provide a simple way to test predictions using real HARTH samples.
* Export a summarized comparison report for viva / submission.

---

### ğŸ§© Algorithms Compared

| Algorithm                    | Type           | Notes                       |
| ---------------------------- | -------------- | --------------------------- |
| Logistic Regression          | Linear         | Baseline classifier         |
| K-Nearest Neighbors (KNN)    | Non-parametric | Distance-based              |
| Naive Bayes                  | Probabilistic  | Simple, fast                |
| Decision Tree                | Non-linear     | Interpretable               |
| Random Forest                | Ensemble       | Robust and stable           |
| Support Vector Machine (SVM) | Kernel-based   | Strong with non-linear data |

---

### ğŸ“‚ Folder Structure

```
ML_Comparison/
â”‚
â”œâ”€â”€ harth70/
â”‚   â”œâ”€â”€ .csv                 # HARTH dataset (or multiple CSVs)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚   â”œâ”€â”€ knn.pkl
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚   â”œâ”€â”€ svm.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ comparison_results.csv    # Metrics summary for all models
â”‚
â”œâ”€â”€ train_and_compare.py          # Core training + evaluation script
â”œâ”€â”€ app.py                        # Streamlit dashboard
â””â”€â”€ README.md                     # Project documentation
```

---

### âš™ï¸ Setup Instructions

#### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

If you donâ€™t have a `requirements.txt`, use:

```bash
pip install streamlit pandas numpy scikit-learn plotly seaborn matplotlib
```

#### 2ï¸âƒ£ Prepare data

Place the HARTH dataset file(s) under:

```
./data/harth.csv
```

Each file should have columns similar to:

```
timestamp, back_x, back_y, back_z, thigh_x, thigh_y, thigh_z, label
```

---

#### 3ï¸âƒ£ Train and Compare Models

Run the training script to generate results:

```bash
python train_and_compare.py
```

This will:

* Train all models
* Save `.pkl` models in `./models`
* Generate `comparison_results.csv` in `./results`

---

#### 4ï¸âƒ£ Launch Streamlit Dashboard

```bash
streamlit run app.py
```

The app provides:

* ğŸ“Š Model performance visualizations (Plotly + Seaborn)
* ğŸ“ˆ Combined metric comparison charts
* ğŸ§ª â€œTry Sample Dataâ€ section for live testing
* ğŸ“„ Downloadable performance report

---

### ğŸ§  Label Mapping

| Label | Activity                  | Notes                      |
| :---- | :------------------------ | :------------------------- |
| 1     | Walking                   | â€”                          |
| 2     | Running                   | â€”                          |
| 3     | Shuffling                 | Standing with leg movement |
| 4     | Stairs (Ascending)        | â€”                          |
| 5     | Stairs (Descending)       | â€”                          |
| 6     | Standing                  | â€”                          |
| 7     | Sitting                   | â€”                          |
| 8     | Lying                     | â€”                          |
| 13    | Cycling (Sit)             | â€”                          |
| 14    | Cycling (Stand)           | â€”                          |
| 130   | Cycling (Sit, Inactive)   | Without leg movement       |
| 140   | Cycling (Stand, Inactive) | Without leg movement       |

---

### ğŸ“Š Key Visualizations

* **Accuracy Comparison** â€” Interactive bar chart (Plotly)
* **Radar Chart** â€” Compare Accuracy, Precision, Recall, F1-Score
* **Heatmap** â€” Visual correlation of model metrics
* **Combined Metrics Chart** â€” Line + bar visualization
* **Training vs Prediction Time Plot**

---

### ğŸ§© Sample Testing UI

In the Streamlit app, you can:

* Select a real sample from the dataset
* Choose a trained model
* Predict activity + view true label and confidence
* Optionally view class probability distribution

Example output:

```
âœ… Predicted Activity: Standing
ğŸ¯ True Label: Standing
ğŸ”¹ Confidence: 94.2%
```

---

### ğŸ“ˆ Example Result Summary (from comparison_results.csv)

| Model         | Accuracy | Precision | Recall | F1-Score | Train Time (s) |
| :------------ | :------- | :-------- | :----- | :------- | :------------- |
| Random Forest | 0.94     | 0.94      | 0.94   | 0.94     | 3.21           |
| SVM           | 0.91     | 0.91      | 0.90   | 0.90     | 5.40           |
| KNN           | 0.88     | 0.88      | 0.87   | 0.87     | 0.15           |
| ...           | ...      | ...       | ...    | ...      | ...            |

---

### ğŸ§° Future Work

* Include all 15 HARTH files for larger-scale evaluation.
* Add **deep learning models (LSTM, CNN)** for comparison.
* Use **cross-validation** for more robust accuracy metrics.
* Optimize model efficiency for **edge deployment on UAV or wearable devices**.

---